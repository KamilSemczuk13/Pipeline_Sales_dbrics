{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12855cce-d1f7-4dcb-8ac6-91cfccdb7150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import all neccesary functions\n",
    "import logging\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d121a3-4631-43b6-9b1b-d71d23f7d248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# logger config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # logs display in console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"ETL_PIPELINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995fff6a-473e-4a3e-9709-2910d37617b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data ingstension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9876ea22-b2e4-4b27-b8b1-45ed52d5320c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file=dbutils.fs.ls(f\"abfss://my-container@storagedbrics.dfs.core.windows.net/project-sales/source\")\n",
    "path=file[0].path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b46e445-0937-4542-8a9f-04b5644e4ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0d1273-6bf5-4755-ba97-066c12f8f1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 13:00:09,037 [INFO] Beginig of Bronze layer file\n",
      "2025-12-23 13:00:09,038 [INFO] File readed succesfully\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Beginig of Bronze layer file\")\n",
    "try:\n",
    "  df=spark.read.format(\"csv\")\\\n",
    "              .option(\"header\", True)\\\n",
    "              .option(\"inferSchema\", True)\\\n",
    "              .load(path)\n",
    "  logger.info(f\"File readed succesfully\")\n",
    "except Exception as e:\n",
    "  logger.error(f\"Error reading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc63cf0-85a1-48c5-b0f0-a8bb9c066450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 13:00:16,548 [INFO] Saving data to bronze layer\n",
      "2025-12-23 13:00:28,455 [INFO] Data saved succesfully to bronze layer\n"
     ]
    }
   ],
   "source": [
    "# write data to Bronze layer\n",
    "\n",
    "logger.info(\"Saving data to bronze layer\")\n",
    "try:\n",
    "    df.write.format(\"delta\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(f\"abfss://my-container@storagedbrics.dfs.core.windows.net/project-sales/datalake/bronze\")\n",
    "    logger.info(\"Data saved succesfully to bronze layer\")\n",
    "except:\n",
    "    logger.error(\"Error during saving data to bronze layer\")\n",
    "# reading bronze layer data to silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b83286-cc57-4b94-9843-0b3b25209db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003d55bf-f9bf-4544-a121-0ca9d434498f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 13:00:28,573 [INFO] Reading data from Bronze layer to Silver layer\n",
      "2025-12-23 13:00:28,574 [INFO] Data succesfully readed from Bronze layer to Silver layer\n"
     ]
    }
   ],
   "source": [
    "# reading bronze layer data to silver\n",
    "\n",
    "\n",
    "logger.info(\"Reading data from Bronze layer to Silver layer\")\n",
    "try:\n",
    "  df_silver=spark.read.format(\"delta\")\\\n",
    "                    .option(\"header\", True)\\\n",
    "                    .option(\"inferSchema\", True)\\\n",
    "                    .load(f\"abfss://my-container@storagedbrics.dfs.core.windows.net/project-sales/datalake/bronze\")\n",
    "  logger.info(\"Data succesfully readed from Bronze layer to Silver layer\")\n",
    "except Exception as e:\n",
    "  logger.error(f\"Error reading file from Bronze layer to Silver layer: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262769a9-0fb9-40e7-b90f-3b9948be58c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of nulls in LK_SO_NUMBER_TXT: 0\n",
      " Number of nulls in LK_SO_ITEM_TXT: 0\n",
      " Number of nulls in GEOHI_ID_LVL1_TXT: 0\n",
      " Number of nulls in GEOHI_ID_LVL2_TXT: 0\n",
      " Number of nulls in GEOHI_ID_LVL3_TXT: 0\n",
      " Number of nulls in GEOHI_ID_LVL4_TXT: 0\n",
      " Number of nulls in LK_CUSTOMER_ID_TXT: 0\n",
      " Number of nulls in LK_MATERIAL_NUMBER_TXT: 0\n",
      " Number of nulls in SAHI_ID_LVL4_TXT: 0\n",
      " Number of nulls in SAHI_ID_LVL5_TXT: 0\n",
      " Number of nulls in SAHI_ID_LVL6_TXT: 0\n",
      " Number of nulls in SAHI_ID_LVL7_TXT: 0\n",
      " Number of nulls in OTDR_EXT_FLG: 0\n",
      " Number of nulls in SI_CPO_CREATION_DAT: 0\n",
      " Number of nulls in SI_SO_CREATION_DAT: 0\n",
      " Number of nulls in READY_TO_SHIP_DAT: 0\n",
      " Number of nulls in CUST_REQ_DELIVERY_DATE_DAT: 0\n",
      " Number of nulls in SI_VENDOR_ID_TXT: 0\n",
      " Number of nulls in VAL_PLANT_ID_TXT: 0\n"
     ]
    }
   ],
   "source": [
    "# counting null values in each column\n",
    "columns=df_silver.columns\n",
    "\n",
    "for column in columns:\n",
    "    null_count=df_silver.filter(col(column).isNull()).count()\n",
    "    print(f\" Number of nulls in {column}: {null_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61f41e1-1005-41f4-8f75-50f51959b1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see that there are no typical nulls values in df. But when we display our df we can clearly see that we have \"NULL\" values in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4245e82-34b9-4643-9cb0-ccbd2d24a96f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of 'null' string types in LK_SO_NUMBER_TXT: 0 \n",
      " Number of 'null' string types in LK_SO_ITEM_TXT: 0 \n",
      " Number of 'null' string types in GEOHI_ID_LVL1_TXT: 2049 \n",
      " Number of 'null' string types in GEOHI_ID_LVL2_TXT: 2049 \n",
      " Number of 'null' string types in GEOHI_ID_LVL3_TXT: 2049 \n",
      " Number of 'null' string types in GEOHI_ID_LVL4_TXT: 2049 \n",
      " Number of 'null' string types in LK_CUSTOMER_ID_TXT: 0 \n",
      " Number of 'null' string types in LK_MATERIAL_NUMBER_TXT: 0 \n",
      " Number of 'null' string types in SAHI_ID_LVL4_TXT: 2049 \n",
      " Number of 'null' string types in SAHI_ID_LVL5_TXT: 2049 \n",
      " Number of 'null' string types in SAHI_ID_LVL6_TXT: 2049 \n",
      " Number of 'null' string types in SAHI_ID_LVL7_TXT: 2049 \n",
      " Number of 'null' string types in OTDR_EXT_FLG: 0 \n",
      " Number of 'null' string types in SI_CPO_CREATION_DAT: 13803 \n",
      " Number of 'null' string types in SI_SO_CREATION_DAT: 0 \n",
      " Number of 'null' string types in READY_TO_SHIP_DAT: 6753 \n",
      " Number of 'null' string types in CUST_REQ_DELIVERY_DATE_DAT: 2855 \n",
      " Number of 'null' string types in SI_VENDOR_ID_TXT: 102983 \n",
      " Number of 'null' string types in VAL_PLANT_ID_TXT: 0 \n"
     ]
    }
   ],
   "source": [
    "# counting nulls in string format\n",
    "\n",
    "for column in df_silver.columns:\n",
    "    null_str_count=df_silver.filter(col(column).cast(StringType()).isin(\"NULL\", \"Null\", \"Nan\", \"nan\")).count()\n",
    "    print(f\" Number of 'null' string types in {column}: {null_str_count} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106f4ac4-912b-47d1-afb8-b0ef65a354f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. changing values in column from string \"Null\" to Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a40129-8ae7-410b-8eee-828ae5706f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138075"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    df_silver=df_silver.withColumn(column, when(trim(lower(col(column))).isin(\"null\"),None).otherwise(col(column)))\n",
    "\n",
    "# count of rows in df_silver with fake nulls\n",
    "df_silver.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642d2f67-ae21-409e-8ef5-b6358a8ce138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Droping Null values, drop duplicates, casting date and timestamp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e2e002-f846-4d83-868a-a5e9e70d34f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver=(\n",
    "    df_silver\n",
    "    .dropna(how=\"all\")\n",
    "    .dropDuplicates()\n",
    "\n",
    "    # Columns casting to timestamp\n",
    "    .withColumn('SI_CPO_CREATION_DAT', to_timestamp(col('SI_CPO_CREATION_DAT'),\"dd/MM/yyyy HH:mm\"))\n",
    "    .withColumn('SI_SO_CREATION_DAT', to_timestamp(col('SI_SO_CREATION_DAT'),\"dd/MM/yyyy HH:mm\"))\n",
    "    .withColumn('READY_TO_SHIP_DAT', to_timestamp(col('READY_TO_SHIP_DAT'),\"dd/MM/yyyy HH:mm\"))\n",
    "    .withColumn('CUST_REQ_DELIVERY_DATE_DAT', to_timestamp(col('CUST_REQ_DELIVERY_DATE_DAT'),\"dd/MM/yyyy HH:mm\")\n",
    "              \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d65b5f7-a9d2-4c50-9a02-ebf5d31a778c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver=(\n",
    "    df_silver\n",
    "    .withColumn(\"DIFF_DATE_CPO_SHIP_DAYS\", date_diff(end=col(\"READY_TO_SHIP_DAT\"), start=col(\"SI_CPO_CREATION_DAT\")))\n",
    "    .withColumn(\"PERIOD\", date_format(col(\"SI_CPO_CREATION_DAT\"), \"yyyy-MM\"))\n",
    "    .withColumn(\"PERIOD\", col(\"PERIOD\").cast(StringType()))\n",
    "    # rename columns\n",
    "    .withColumnRenamed(\"GEOHI_ID_LVL4_TXT\", \"COUNTRY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da8c072-cb25-40bb-ae34-677e0b2a9dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 13:00:49,622 [INFO] Saving data to silver layer\n",
      "2025-12-23 13:00:54,005 [INFO] Data succesfully saved to silver layer\n"
     ]
    }
   ],
   "source": [
    "# writing data to silver Layer\n",
    "\n",
    "logger.info(\"Saving data to silver layer\")\n",
    "try:\n",
    "    df_silver.write.format(\"delta\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .save(f\"abfss://my-container@storagedbrics.dfs.core.windows.net/project-sales/datalake/silver\")\n",
    "    logger.info(\"Data succesfully saved to silver layer\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during saving data to silver layer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a112e48-4b3e-46c3-b98e-9c08597da8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GOLD LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7401fb2-565d-4452-bd61-2bce9503756d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 13:00:54,142 [INFO] Reading data from silver to gold layer\n",
      "2025-12-23 13:00:54,143 [INFO] Data succesfully readed from silver to gold layer\n"
     ]
    }
   ],
   "source": [
    "# reading data from silver to gold layer\n",
    "\n",
    "logger.info(\"Reading data from silver to gold layer\")\n",
    "try:\n",
    "    df_gold=spark.read.format(\"delta\")\\\n",
    "        .load(f\"abfss://my-container@storagedbrics.dfs.core.windows.net/project-sales/datalake/silver\")\n",
    "    logger.info(\"Data succesfully readed from silver to gold layer\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading data from silver to gold layer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43db5777-573f-4d93-91f4-838cf7b11174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transforming and agregating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a066b7-809a-476a-be00-22682fce5eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_grouped=df_gold.groupBy(\"COUNTRY\",\"PERIOD\").agg(\n",
    "    sum(col(\"OTDR_EXT_FLG\")).alias(\"SUM_OTDR\"),\n",
    "    count(col(\"OTDR_EXT_FLG\")).alias(\"TOTAL_OTDR\")\n",
    ")\n",
    "\n",
    "df_gold_grouped=df_gold_grouped.withColumn(\"PTC_OTDR\", round((col(\"SUM_OTDR\")/col(\"TOTAL_OTDR\"))*100,2) )\n",
    "df_gold_grouped=df_gold_grouped.drop(\"SUM_OTDR\",\"TOTAL_OTDR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c111adbb-1807-4eaa-a7aa-d837463aa012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 13:00:54,929 [INFO] Saving data to gold layer\n",
      "2025-12-23 13:00:57,659 [INFO] Data succesfully saved to gold layer\n"
     ]
    }
   ],
   "source": [
    "# writing data to gold layer\n",
    "\n",
    "logger.info(\"Saving data to gold layer\")\n",
    "try:\n",
    "    df_gold_grouped.write.format(\"delta\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(f\"abfss://my-container@storagedbrics.dfs.core.windows.net/project-sales/datalake/gold\")\n",
    "    logger.info(\"Data succesfully saved to gold layer\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during saving data to gold layer: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4609998189275783,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Sales_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
